{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-09T09:29:28.897419Z",
     "iopub.status.busy": "2022-03-09T09:29:28.896887Z",
     "iopub.status.idle": "2022-03-09T09:29:28.948042Z",
     "shell.execute_reply": "2022-03-09T09:29:28.947243Z",
     "shell.execute_reply.started": "2022-03-09T09:29:28.897322Z"
    },
    "papermill": {
     "duration": 0.230891,
     "end_time": "2021-03-08T07:57:06.335029",
     "exception": false,
     "start_time": "2021-03-08T07:57:06.104138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import random\n",
    "import io # Input/Output Module\n",
    "import os # OS interfaces\n",
    "import cv2 # OpenCV package\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from urllib import request # module for opening HTTP requests\n",
    "from matplotlib import pyplot as plt # Plotting library\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# For Data Augmentation\n",
    "from torchvision import transforms as tr\n",
    "from torchvision.transforms import Compose\n",
    "from PIL import Image\n",
    "\n",
    "# For VGG Classifier\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022868,
     "end_time": "2021-03-08T07:57:06.382109",
     "exception": false,
     "start_time": "2021-03-08T07:57:06.359241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 1\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r0876133, r0876755, r3, r4, r5</span>.\n",
    "\n",
    "The goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n",
    "\n",
    "In this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n",
    "\n",
    "---------------------------------------------------------------\n",
    "This notebook is structured as follows:\n",
    "\n",
    "0. Data loading & Preprocessing\n",
    "1. Feature Representations\n",
    "2. Evaluation Metrics \n",
    "3. Classifiers\n",
    "4. Experiments\n",
    "5. Publishing best results\n",
    "6. Discussion\n",
    "\n",
    "Make sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n",
    "\n",
    "Fill in your student numbers above and get to it! Good luck! \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n",
    "</div>\n",
    "    \n",
    "---------------------------------------------------------------\n",
    "# 0. Data loading & Preprocessing\n",
    "\n",
    "## 0.1. Loading data\n",
    "The training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T09:29:28.954193Z",
     "iopub.status.busy": "2022-03-09T09:29:28.953352Z",
     "iopub.status.idle": "2022-03-09T09:29:32.840002Z",
     "shell.execute_reply": "2022-03-09T09:29:32.839124Z",
     "shell.execute_reply.started": "2022-03-09T09:29:28.954147Z"
    },
    "papermill": {
     "duration": 37.543619,
     "end_time": "2021-03-08T07:57:43.9495",
     "exception": false,
     "start_time": "2021-03-08T07:57:06.405881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The training set contains 80 examples, the test set contains 1816 examples.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "\n",
    "train = pd.read_csv(\n",
    "    './kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train_set.csv', index_col = 0)\n",
    "train.index = train.index.rename('id')\n",
    "\n",
    "test = pd.read_csv(\n",
    "    './kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test_set.csv', index_col = 0)\n",
    "test.index = test.index.rename('id')\n",
    "\n",
    "# read the images as numpy arrays and store in \"img\" column\n",
    "train['img'] = [cv2.cvtColor(np.load('./kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in train.iterrows()]\n",
    "\n",
    "test['img'] = [cv2.cvtColor(np.load('./kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n",
    "                for index, row in test.iterrows()]\n",
    "  \n",
    "\n",
    "train_size, test_size = len(train),len(test)\n",
    "\n",
    "\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023377,
     "end_time": "2021-03-08T07:57:43.997466",
     "exception": false,
     "start_time": "2021-03-08T07:57:43.974089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n",
    "\n",
    "## 0.2. A first look\n",
    "Let's have a look at the data columns and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T09:29:32.841625Z",
     "iopub.status.busy": "2022-03-09T09:29:32.841363Z"
    },
    "papermill": {
     "duration": 3.315629,
     "end_time": "2021-03-08T07:57:47.336913",
     "exception": false,
     "start_time": "2021-03-08T07:57:44.021284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training set contains an identifier, name, image information and class label\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.283501,
     "end_time": "2021-03-08T07:57:50.644778",
     "exception": false,
     "start_time": "2021-03-08T07:57:47.361277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The test set only contains an identifier and corresponding image information.\n",
    "\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.046628,
     "end_time": "2021-03-08T07:57:50.716317",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.669689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The class distribution in the training set:\n",
    "train.groupby('name').agg({'img':'count', 'class': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025108,
     "end_time": "2021-03-08T07:57:50.766719",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.741611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n",
    "\n",
    "## 0.3. Preprocess data\n",
    "### 0.3.1 Example: HAAR face detector\n",
    "In this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.042776,
     "end_time": "2021-03-08T07:57:50.834913",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.792137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HAARPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, path, face_size):\n",
    "        self.face_size = face_size\n",
    "        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n",
    "        if not os.path.exists(file_path): \n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.download_model(file_path)\n",
    "        \n",
    "        self.classifier = cv2.CascadeClassifier(file_path)\n",
    "  \n",
    "    def download_model(self, path):\n",
    "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n",
    "            \"haarcascades/haarcascade_frontalface_default.xml\"\n",
    "        \n",
    "        with request.urlopen(url) as r, open(path, 'wb') as f:\n",
    "            f.write(r.read())\n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"        \n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        norm_img = np.zeros((FACE_SIZE[0], FACE_SIZE[1]))\n",
    "        img_gray = cv2.normalize(img_gray, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "        return self.classifier.detectMultiScale(\n",
    "            img_gray,\n",
    "            scaleFactor=1.25,\n",
    "            minNeighbors=4,\n",
    "            minSize=(40, 40),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE  \n",
    "        )\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        \n",
    "        faces = self.detect_faces(img)\n",
    "\n",
    "        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        # only return the first face\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNPreprocessor improves the HAARPreprocessor. Probably best performing face detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNPreprocessor():\n",
    "    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n",
    "    \n",
    "    def __init__(self, face_size):\n",
    "        self.face_size = face_size\n",
    "        self.modelFile = \"./models/opencv_face_detector.caffemodel\"\n",
    "        self.configFile = \"./models/deploy.prototxt.txt\"\n",
    "        self.net = cv2.dnn.readNetFromCaffe(self.configFile, self.modelFile)\n",
    "        self.default_confidence = 0.5\n",
    "    \n",
    "            \n",
    "    def detect_faces(self, img):\n",
    "        \"\"\"Detect all faces in an image.\"\"\"\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0,(300, 300), (104.0, 117.0, 123.0))\n",
    "        self.net.setInput(blob)\n",
    "        return self.net.forward()\n",
    "        \n",
    "    def extract_faces(self, img):\n",
    "        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n",
    "        confidence = self.default_confidence\n",
    "        faces = []\n",
    "        h, w = img.shape[:2]\n",
    "        f = self.detect_faces(img)\n",
    "        #to draw faces on image\n",
    "        for i in range(f.shape[2]):\n",
    "                c = f[0, 0, i, 2]\n",
    "                if c > confidence and confidence == 0.5:\n",
    "                    faces.append((f[0, 0, i, 3:7] * np.array([w, h, w, h])).astype(\"int\"))\n",
    "                    confidence = c\n",
    "                elif c > confidence and confidence != 0.5:\n",
    "                    faces.pop()\n",
    "                    faces.append((f[0, 0, i, 3:7] * np.array([w, h, w, h])).astype(\"int\"))\n",
    "                    confidence = c\n",
    "                    \n",
    "        return [img[y:y1, x:x1]  for (x, y, x1, y1) in faces]\n",
    "    \n",
    "    def preprocess(self, data_row):\n",
    "        faces = self.extract_faces(data_row['img'])\n",
    "        \n",
    "        # if no faces were found, return None\n",
    "        if len(faces) == 0:\n",
    "            nan_img = np.empty(self.face_size + (3,))\n",
    "            nan_img[:] = np.nan\n",
    "            return nan_img\n",
    "        # only return the first face\n",
    "        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    def __call__(self, data):\n",
    "        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025332,
     "end_time": "2021-03-08T07:57:50.885849",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.860517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Visualise**\n",
    "\n",
    "Let's plot a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "papermill": {
     "duration": 62.263517,
     "end_time": "2021-03-08T07:58:53.174859",
     "exception": false,
     "start_time": "2021-03-08T07:57:50.911342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter to play with \n",
    "FACE_SIZE = (224, 224)\n",
    "\n",
    "def plot_image_sequence(data, n, imgs_per_row=7):\n",
    "    n_rows = 1 + int(n/(imgs_per_row+1))\n",
    "    n_cols = min(imgs_per_row, n)\n",
    "\n",
    "    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n",
    "    for i in range(n):\n",
    "        if n == 1:\n",
    "            ax.imshow(data[i].astype('uint8'))\n",
    "        elif n_rows > 1:\n",
    "            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i].astype('uint8'))\n",
    "        else:\n",
    "            ax[int(i%n)].imshow(data[i].astype('uint8'))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#preprocessed data \n",
    "preprocessor = DNNPreprocessor(face_size=FACE_SIZE)\n",
    "\n",
    "train_modified = copy.deepcopy(train)\n",
    "\n",
    "# #train_set preprocessing to improve the performance of the face_detector.\n",
    "# #Double face in a single image are removed. \n",
    "train_img = train_modified['img'].values\n",
    "train_img[5] = train_img[5][0:400, :]\n",
    "train_img[70] = train_img[70][:, 150:]\n",
    "train_img[61] = train_img[61][:, 100:]\n",
    "train_img[60] = train_img[60][0:1500,:]\n",
    "train_img[59] = train_img[59][:, 200:]\n",
    "train_img[53] = train_img[53][:, 250:]\n",
    "train_img[52] = train_img[52][:, 300:]\n",
    "train_img[50] = train_img[50][0:300,:]\n",
    "train_img[49] = train_img[49][:, 0:150]\n",
    "train_img[41] = train_img[41][:, 0:200]\n",
    "train_img[39] = train_img[39][:, 125:225]\n",
    "train_img[40] = train_img[40][:, 100:]\n",
    "train_img[34] = train_img[34][:, 200:400]\n",
    "train_img[29] = train_img[29][:, :400]\n",
    "train_img[28] = train_img[28][:, 0:500]\n",
    "train_img[18] = train_img[18][:, :250]\n",
    "\n",
    "train_X, train_y = preprocessor(train_modified), train['class'].values\n",
    "\n",
    "#Image65 is wrong (total black), so it is eliminated\n",
    "train_X = np.delete(train_X, 65, axis=0)\n",
    "train_y = np.delete(train_y, 65, axis=0)\n",
    "\n",
    "test_X = preprocessor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.635787,
     "end_time": "2021-03-08T07:58:55.836611",
     "exception": false,
     "start_time": "2021-03-08T07:58:53.200824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot faces of Michael and Sarah\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 0], n=train_X[train_y == 0].shape[0], imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.840961,
     "end_time": "2021-03-08T07:58:59.72249",
     "exception": false,
     "start_time": "2021-03-08T07:58:55.881529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot faces of Jesse\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 1], n=train_X[train_y == 1].shape[0], imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.910256,
     "end_time": "2021-03-08T07:59:03.703299",
     "exception": false,
     "start_time": "2021-03-08T07:58:59.793043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot faces of Mila\n",
    "\n",
    "plot_image_sequence(train_X[train_y == 2], n=train_X[train_y == 2].shape[0], imgs_per_row=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100995,
     "end_time": "2021-03-08T07:59:03.904684",
     "exception": false,
     "start_time": "2021-03-08T07:59:03.803689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 0.4. Store Preprocessed data (optional)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n",
    "</div>\n",
    "\n",
    "Data after data augmentation can be stored using the code in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.109823,
     "end_time": "2021-03-08T07:59:04.11528",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.005457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prep_path = './kaggle/working/prepped_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data\n",
    "# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n",
    "# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n",
    "# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n",
    "# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n",
    "# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation: image flip\n",
    "\n",
    "The plot below explains the flip operation. This occurs when the probability is between 0 and 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_X[9]]\n",
    "data.append(cv2.flip(train_X[9], 1))\n",
    "\n",
    "plot_image_sequence(data, 2, imgs_per_row=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation: image rotation\n",
    "\n",
    "The plot below shows the rotation operation. This occurs when the probability is between 0.3 and 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Compose(\n",
    "             [tr.RandomRotation(degrees = 90),\n",
    "              tr.RandomRotation(degrees = 270)])\n",
    "\n",
    "image = train_X[9]\n",
    "image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "augmented_image = pipeline(img = image)\n",
    "\n",
    "data = [image, augmented_image]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.imshow(augmented_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation: final operation\n",
    "\n",
    "The given code snippet below augments the given data, with the flip and rotation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Compose(\n",
    "             [tr.RandomRotation(degrees = 90),\n",
    "              tr.RandomRotation(degrees = 270)])\n",
    "\n",
    "# augmented_image = pipeline(img = img)\n",
    "\n",
    "print(len(train_X))\n",
    "augm_train_X = []\n",
    "augm_train_y = []\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "for idx, im in enumerate(train_X):\n",
    "    augm_train_X.append(im)\n",
    "    augm_train_y.append(train_y[idx])\n",
    "    \n",
    "    rand = random.uniform(0, 1)\n",
    "    \n",
    "    if rand < threshold:\n",
    "        flipped = cv2.flip(im, 1)\n",
    "        augm_train_X.append(flipped)\n",
    "        augm_train_y.append(train_y[idx])\n",
    "    elif rand > threshold and rand < (threshold * 2):\n",
    "        image = Image.fromarray(np.uint8(im)).convert('RGB')\n",
    "        augmented_image = pipeline(img = image)\n",
    "        augm_train_X.append(np.array(image))\n",
    "        augm_train_y.append(train_y[idx])\n",
    "\n",
    "print(len(augm_train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(augm_train_X)\n",
    "train_y = np.array(augm_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100101,
     "end_time": "2021-03-08T07:59:04.315571",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.21547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we are ready to rock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Classifier (WIP)\n",
    "\n",
    "Load the weights of VGG16 and freeze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "img_rows, img_cols = 224, 224\n",
    "\n",
    "model = vgg16.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new layers for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_adder(vgg, classes):\n",
    "    out = vgg.output\n",
    "    out = GlobalAveragePooling2D()(out)\n",
    "    out = Dense(1024,activation='relu')(out)\n",
    "    out = Dense(1024,activation='relu')(out)\n",
    "    out = Dense(512,activation='relu')(out)\n",
    "    out = Dense(classes,activation='softmax')(out)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3\n",
    "\n",
    "head = layer_adder(model, classes)\n",
    "\n",
    "final_model = Model(inputs = model.input, outputs = head)\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As collected data is very small to train the model, we use the concept of augmented images to expans the dataset by using methods like crop, resize, zoom, rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blurr every image (grayscaling gives error - have to look into that)\n",
    "train_X = train_X.astype(np.uint8)\n",
    "\n",
    "preprocessed = []\n",
    "for img in train_X:\n",
    "    blurred = cv2.GaussianBlur(img, (5,5), 0)\n",
    "#     gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    preprocessed.append(blurred)\n",
    "\n",
    "preprocessed = np.array(preprocessed)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# This functionn will change the current file to, e.g. [0, 0, 1] if the label is 2\n",
    "preprocessed_train_y = to_categorical(train_y, 3)\n",
    "\n",
    "# 70% goes to training, 30% goes to validating\n",
    "chunked = int(len(train_X) * 0.7)\n",
    "\n",
    "vgg_train_X = preprocessed[:chunked]\n",
    "vgg_validate_X = preprocessed[chunked:]\n",
    "\n",
    "vgg_train_y = preprocessed_train_y[:chunked]\n",
    "vgg_validate_y = preprocessed_train_y[chunked:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.3)\n",
    "\n",
    "datagen.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# This functionn will change the current file to, e.g. [0, 0, 1] if the label is 2\n",
    "preprocessed_train_y = to_categorical(train_y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6441 - accuracy: 0.6964 - val_loss: 1.3275 - val_accuracy: 0.4783\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.7265 - accuracy: 0.6607 - val_loss: 0.6776 - val_accuracy: 0.6957\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.4313 - accuracy: 0.8571 - val_loss: 0.7523 - val_accuracy: 0.7391\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.3705 - accuracy: 0.8036 - val_loss: 0.5078 - val_accuracy: 0.8261\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2779 - accuracy: 0.9286 - val_loss: 0.4114 - val_accuracy: 0.7826\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2953 - accuracy: 0.8571 - val_loss: 0.6775 - val_accuracy: 0.7391\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.3186 - accuracy: 0.8750 - val_loss: 0.5674 - val_accuracy: 0.7391\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.5143 - accuracy: 0.7321 - val_loss: 0.5582 - val_accuracy: 0.7826\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2801 - accuracy: 0.8750 - val_loss: 0.4811 - val_accuracy: 0.8261\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2287 - accuracy: 0.8929 - val_loss: 0.3975 - val_accuracy: 0.7826\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2662 - accuracy: 0.8929 - val_loss: 0.6617 - val_accuracy: 0.7826\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2922 - accuracy: 0.8571 - val_loss: 0.8010 - val_accuracy: 0.7391\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2223 - accuracy: 0.9107 - val_loss: 0.4099 - val_accuracy: 0.8261\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1451 - accuracy: 0.9286 - val_loss: 0.4930 - val_accuracy: 0.7391\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1637 - accuracy: 0.9286 - val_loss: 0.3125 - val_accuracy: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Compiles the model\n",
    "# Loss functions that can be used: 'kl_divergence', 'poisson', 'categorical_crossentropy', 'sparse_categorical_crossentropy'\n",
    "opt = Adam(learning_rate=0.001)\n",
    "final_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(vgg_train_X, vgg_train_y, epochs=7, verbose=1, validation_data=(vgg_validate_X, vgg_validate_y))\n",
    "history = final_model.fit(datagen.flow(train_X, preprocessed_train_y, batch_size=16, subset='training'),\n",
    "         validation_data=datagen.flow(train_X, preprocessed_train_y, batch_size=16, subset='validation'),\n",
    "          epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABNOUlEQVR4nO3dd3hUZfbA8e9JJwECIbSE0DuE0EEERQFFqhQFrIjiYmEt666uumvfdVd31/pDUbEiiAUEpEgTUSDUhI5AKAmhhIQW0pP398cdMISUSTKTmWTO53l4kpm5c+9JmNxz71vOK8YYlFJKeS4vVweglFLKtTQRKKWUh9NEoJRSHk4TgVJKeThNBEop5eE0ESillIfTRKA8ioh8IiIv27ntIREZ6OyYlHI1TQRKKeXhNBEoVQmJiI+rY1BVhyYC5XZsTTJ/FpFtInJBRD4SkfoislhEzovIchGpnW/7ESKyU0TOiMhPItIu32tdRGSL7X1fAQEFjjVMRGJs710rIp3sjHGoiGwVkXMiEi8izxd4va9tf2dsr0+0PV9NRP4jIodF5KyI/GJ7rr+IJBTyexho+/55EflGRL4QkXPARBHpKSLrbMc4JiLviIhfvvd3EJFlIpIiIidE5GkRaSAiaSJSJ9923UQkSUR87fnZVdWjiUC5qzHAIKA1MBxYDDwNhGJ9bv8IICKtgVnAo0BdYBGwQET8bCfFecDnQAjwtW2/2N7bFZgB/AGoA7wPzBcRfzviuwDcBdQChgIPiMjNtv02tsX7ti2mzkCM7X2vA92APraY/gLk2fk7GQl8YzvmTCAXeAzrd3IVMAB40BZDDWA5sAQIA1oCK4wxx4GfgFvz7fcOYLYxJtvOOFQVo4lAuau3jTEnjDFHgTVAtDFmqzEmE5gLdLFtNw74wRizzHYiex2ohnWi7Q34Am8YY7KNMd8AG/MdYzLwvjEm2hiTa4z5FMi0va9YxpifjDHbjTF5xphtWMnoWtvLtwPLjTGzbMdNNsbEiIgXMAl4xBhz1HbMtbafyR7rjDHzbMdMN8ZsNsasN8bkGGMOYSWyizEMA44bY/5jjMkwxpw3xkTbXvsU6+SPiHgDE7CSpfJQmgiUuzqR7/v0Qh5Xt30fBhy++IIxJg+IB8Jtrx01l1dWPJzv+ybAn2xNK2dE5AwQYXtfsUSkl4issjWpnAWmYF2ZY9vHgULeForVNFXYa/aILxBDaxFZKCLHbc1F/7AjBoDvgfYi0hzrruusMWZDGWNSVYAmAlXZJWKd0AEQEcE6CR4FjgHhtucuapzv+3jgFWNMrXz/Ao0xs+w47pfAfCDCGBMMvAdcPE480KKQ95wCMop47QIQmO/n8MZqVsqvYKngacAeoJUxpiZW01lJMWCMyQDmYN253IneDXg8TQSqspsDDBWRAbbOzj9hNe+sBdYBOcAfRcRHREYDPfO99wNgiu3qXkQkyNYJXMOO49YAUowxGSLSE7gt32szgYEicqvtuHVEpLPtbmUG8F8RCRMRbxG5ytYn8RsQYDu+L/AsUFJfRQ3gHJAqIm2BB/K9thBoICKPioi/iNQQkV75Xv8MmAiMAL6w4+dVVZgmAlWpGWP2YrV3v411xT0cGG6MyTLGZAGjsU54p7H6E77L995NWP0E79he32/b1h4PAi+KyHng71gJ6eJ+jwBDsJJSClZHcZTt5SeA7Vh9FSnAvwAvY8xZ2z4/xLqbuQBcNoqoEE9gJaDzWEntq3wxnMdq9hkOHAf2Adfle/1XrE7qLbb+BeXBRBemUcozichK4EtjzIeujkW5liYCpTyQiPQAlmH1cZx3dTzKtbRpSCkPIyKfYs0xeFSTgAK9I1BKKY+ndwRKKeXhKl3hqtDQUNO0aVNXh6GUUpXK5s2bTxljCs5NASphImjatCmbNm1ydRhKKVWpiMjhol7TpiGllPJwmgiUUsrDaSJQSikPp4lAKaU8nCYCpZTycJoIlFLKw2kiUEopD1fp5hEo95OamcPyXScYHhWGt5eU/AalqqgLmTks23WCuKRUp+y/e9MQrmld6JywctFEoMrtnZX7eW/1ARLPpvNg/5auDkepCpWTm8evB5KZuyWBpTtPkJ6dC4A44ZpoyrUtNBEo95OVk8c3m+Px9hL+t+w3rm1dlw5hwa4OSymnMsaw4+g55m49yvzYRE6lZhJczZdRXcMZ1SWc7k1qI87IBE6iiUCVy4rdJziVmsV/b43i1cV7eOyrGOY/3JcAX29Xh6aUw8WnpDE/NpG5W4+y/2Qqft5eXN+2HqO6htO/TV38fSrn514TgSqXWRvjaRgcwMjO4YQE+THx4428vnQvzw5r7+rQlHKIs2nZLNpxjLlbjrLhUAoAPZuG8I9RkQyNbEhwoK+LIyw/TQSqzOJT0lizL4k/Xt8Kby+hf5t63Nm7CR/9epAB7epzVYs6rg5RqTLJzMnlp71JzN1ylJV7TpKVm0fzukE8cUNrRnYOJyIk0NUhOpQmAlVmczbFA3Brj4hLz/11SFt+2X+KJ76OZfGj/agZUPmvlpRnMMaw6fBp5m49yg/bjnE2PZvQ6n7c3rsxo7s0omN4zUrV7l8amghUmeTk5jFnUzz9W9clvFa1S88H+vnw31ujGPveOp6fv5P/3trZdUEqZYcDSanM23qUuVuPknA6nWq+3tzYoT43dwmnb8tQfLyr/nQrTQSqTFbtTeLEuUxeHNn4ite6NK7NQ9e15K0V+xjUrj43RTZ0QYRKFW/rkdM8N38n2xLO4iVwdctQHh/Umhs6NKC6v2edGj3rp1UOM3vDEerV8Of6tvUKfX3q9S35ae9Jnp67nW5NalOvZkAFR6hU0XYfO8fdMzZQI8CXZ4e2Y3hUGPU9+DNa9e95lMMdO5vOqr0nuaV7I3yLuG329fbiv7d2Ji0rlye/3YYxpoKjVKpwh5MvcNeMDQT6+fDVH3pzX7/mHp0EwMmJQEQGi8heEdkvIk8V8nptEZkrIttEZIOIdHRmPMox5mxMIM/A+B5XNgvl17JedZ4e0o5Ve5P4csORCopOqaKdPJfBnR9tIDs3j8/v7Umj2lVr9E9ZOS0RiIg38C5wE9AemCAiBQeXPw3EGGM6AXcBbzorHuUYuXmGrzYeoV+rULuG0N3Zuwn9WoXy8sLdHDp1oQIiVKpwZ9OyuWvGBk6lZvLJPT1pVb+Gq0NyG868I+gJ7DfGxBljsoDZwMgC27QHVgAYY/YATUWkvhNjUuX0874kEs9mMKFn8XcDF3l5Ca+NjcLPx4vH5sSQk5vn5AiVulJaVg6TPt1IXNIFpt/Znc4RtVwdkltxZiIIB+LzPU6wPZdfLDAaQER6Ak2ARgV3JCL3i8gmEdmUlJTkpHCVPWZvOEKdID8GtrM/XzcIDuClmzuy9cgZ3lt9wInRKXWlrJw8HvhiC1uPnObN8Z3p2yrU1SG5HWcmgsJmXhTsMXwVqC0iMcBUYCuQc8WbjJlujOlujOlet67jK+8p+5w8l8Hy3ScZ260Rfj6l++iMiApjRFQYbyzfx/aEs06KUKnL5eUZnvg6ltW/JfHKqEgdylwEZw4fTQAi8j1uBCTm38AYcw64B0CsKXsHbf+UG/p6cwK5eYZxPSJK3rgQL43syIaDKTw2J4aFU7UwnStk5eTx096TzIuxiqY5miAM7tiAh69vWeSIsopijOG5+TuZH5vIk4Pb2t2c6YmcmQg2Aq1EpBlwFBgP3JZ/AxGpBaTZ+hDuA362JQflZvLyDLM3HqF38xCa161epn0EB/ry2i2duPOjDfxryR6eG97BwVGqwhhj2HyxdML2Y5xJy6ZOkB/dmtR2+EJCZ9OzeXPFPn7el8Sb47rQuI7rRuX8b/k+Pl9/mD9c05wH+rdwWRyVgdMSgTEmR0QeBpYC3sAMY8xOEZlie/09oB3wmYjkAruAe50VjyqftQeSiU9J54kb2pRrP/1a1WVin6Z8/OshBrarz9Uttb3WWeIulk6IOUp8SjoBvl7c0L4Bo7papROcdcW+IDaRp+duZ8hba3hlVEdGdi7YNeh8H/96kLdW7OPW7o146qa2FX78ykYq20Sf7t27m02bNrk6DI/z0Mwt/HrgFOv/OqDcTTrpWbkMfXsN6Vm5LHn0GoKraWE6RzmVmsnC2ETmxiQSG3/mUumEmzuHc2PHiiudEJ+SxqNfxbD58GlGdwnnxZs7Vtixv9uSwONzYrmxQ33eva2rR9QKsoeIbDbGdC/sNS0xoUp0KjWTH3cd566rmjqkXb+anzdvjOvM6P9by3Pf7+CN8V0cEKXnSs/KZdnuE8zbepTVvyWRm2do37Amzwxpx4jOrimdEBESyFf39+btlft5e+U+Nh85zZvjuzh92ObyXSf48zfb6NOiDm+O76JJwE6aCFSJvt2cQHauYULPsnUSF6ZTo1pMvb4V/1v+GwPb12dYpzCH7dsT5OYZ1sclM3frUZbsOE5qZg4NgwOY3K85o7qE06aB6ydL+Xh78dig1lzdMpRHZ29l7LS1PH5Da6Zc0wIvB/dNAETHJfPQl1voEFaT6Xd118EIpaCJQBXLGMPsjfF0b1KblvUce3J56LoWrNx7kmfm7qB7kxAaBHt2vRd77D52jnlbj/J9TCLHz2VQw9+HIZENuLlLOL2b1XHKCba8ejYLYfEj1/D03O38e8leftl3iv/e2tmh/987jp7lvk83EV67Gp/c09PjqoeWl/YRqGKtO5DMhA/W859bohjT7Yq5fuUWl5TKkLfW0LNZHT69p0eVXfijPI6dTWd+jLVO7p7j5/HxEvq3qcuoLo0Y0K5epbnyNcYwZ1M8z8/fRYCvF/8a04kbOjQo934PnrrALe+txc/bi28e6ENYvvUx1O+0j0CV2eyNR6gR4MMQJ03EaV63Os8Mbc/f5u3gi/WHufOqpk45jjGGXcfOMXfLURbvOM74HhFMHdDKKcdypFV7TnLvpxvJM9ClcS1eGtmBoZ3CCAnyc3VopSYijOvRmO5NQ/jjrK3c//lm7uzdhGeGtitzMjt+NoM7Powmz8Dn9/XSJFBGmghUkU5fyGLxjuNM6BFBNT/nXXXe0asxy3ed4JVFu+nTMpQWZZynUJijZ9L5PuYo87Ye5bcTqfh6C8HV/Phs/WEevK6lw8fRO9qn6w7RMLgaM+/rRdPQIFeH4xAt6lbnuwf78PrSvXyw5iDRB5N5a0IX2jaoWar9nL6QxZ0fRXMmLYvZ91/l0M+Np9EudVWk77YeJSsnj/FOnpEpIvx7bCcCfL15fE5suQvTncvI5quNRxg/fR1Xv7qSfy/ZS80AX16+uSMbnh7I8yPak3Q+k+i4ZAf9BM6RciGLX/adYkTnsCqTBC7y9/HmmaHt+XRST1IuZDPinV/5dO0hu9etuJCZwz2fbORwShof3N2dyEbBTo7YDSx8HPb84JRd6x2BKpQxhtkbjtA5ohbtGpbuSq0s6tcM4JWbI3noyy28u+oAjwwsXbNNVk4eq39LYt7WoyzbfYKsnDyahwbx+KDW3Nw5/LIZrgPa1ifIz5v5sYn0ceMJbYu2HyMnzzC8Co+ourZ1XZY82o8nvo7lufk7WbMviX+PjSq26SszJ5cpX2xmW8IZpt3RjT4t3Pf/0GGS9sKmjyCkmVN2r4lAFWrz4dPsO5nKv8ZEVtgxh3ZqyPLd4by1ch/929QlqoQx58YYthw5w7ytR1m4LZHTttIJt/VszKgu4XRqFFxo53M1P28Gta/P4h3HeXFkx1IX0KsoC2ITaVmvOu0aun4oqDOFVvfn44k9+PjXQ7y6eA+D3/iZ/95aeJXQ3DzD41/FsmbfKf49thM3OqCzuVKImQniDZ3GOWX3mghUoWZtiKe6v0+Fj+9/fkQH1scl89icGH6Y2q/QvomDpy4wb+tR5sUc5XByGv4+XtzQoQGju4TTt5V9pROGR4UxLyaRNfuSGFCKktoV5fjZDDYcSuGxga09YiSViDCpbzN6N6/D1FlbuHNGNPdf05w/DWpzKVEbY3h23g5+2H6MZ4a049bujpvX4tZycyB2NrS+EaoXvkZ4eWkiUFc4m57ND9sTGd21EUEVPB47uJov/7klits+jObVxbt5YaS1emlyaiY/bD/Gd1uOEhN/BhHo06IOU69vxY0d6lMjoHRlKvq1qktwNV/mxya6ZSJYuC0RY6yE5Unah9Vk4dR+vLhwF++vjmPdgWTeHN+FZqFBvLZ0L7M2HOHB/i2YfE1zV4dacfYvh9QT0Pl2px1CE4GbysjOxd/HyyVXg9/HHCUjO48JJaxJ7Cx9WoYy6epmzPj1IKHV/YmJP8Pq35LIyTO0bVCDp4e0ZURUeLkmJPn5eDEksgHfxySSnpXr1FFRZTE/NpHI8GCaVbFOYntU8/Pmn6MjubZ1KE9+u52hb63hxg4NmLv1KBN6NubPN5av8GGlE/MFBNW17gicRBOBmxr3/jq8vYTP7+1VoVflxhi+jD5Cx/CaLh2J8ZfBbVizL4n/LPuNBjUDuLdfM27uHO7QjuvhUWHM2hDPij0n3KrExaFTF9iWcJZnhrRzdSguNbhjQzo1qsVjX8Uwd+tRhkY25OWbO3pEU9klF5Jh7xLo9Qfwdl5xRk0Ebuj42Qxibat4/eHzzXw0sTv+PhVzxRqbcJY9x8/z8s0dK+R4RQnw9eaL+3pxODnNKXXzAXo1q0O9Gv7Mj0l0q0SwIDYRERgWpatphdWqxpeTexN9MJnuTULcft6Hw22fA3nZTm0WAp1H4JbW28a339u3Gb/sP8VjX8WQm1cxpUBmbzhCNV9vRnZ2/Ymxfs0AejZz3h+/t5cwtFNDftqbxNn0bKcco7SMMcyPTaRH0xAaBussWbD+n/q0CHXb0V1OYwxs/QLCukD99k49lIf9ZiuHdQeSqRngw9ND2vHs0HYs2n6cZ+dtt3uyTVmlZuYwPzaR4VENS935WlmNiAojKzePpTuPuzoUAPYcP8++k6ke10msCnEsFk7scPrdAGgicEvrDybTq3kdvL2E+/o156HrWjBrQzz/XrrXqcedH5NIWlauR63t2jmiFhEh1VgQm1jyxhVgfmwi3l7CkI4eMj5eFS1mJnj7Q+RYpx9KE4GbSTyTzuHkNHo3r3PpuSduaMNtvRoz7acDTP/5gNOOPWvDEdo2qOH0xUPciYgwvFMYv+4/xanUTJfGYoxhQWwifVuGUqe6v0tjUS6Wkwnbv4Z2w6BabacfThOBm1l3wOofuCpfIhARXhrZkaGdGvKPRXuYsyne4cfdcfQs24+eZXyPCM8alQGM6BxGnrFKOrjS1vgzJJxOZ4Q2C6m9iyD9dIU0C4EmArezPi6ZWoG+tC2wwpS3l/C/WzvTr1UoT327zeFt2rM2HMHfx4tRXRy/5oC7a9ugJq3rV2d+jGubh+bHJOLn48UNHdxvgpuqYFu/gJrh0Lx/hRxOE4GbWReXTK9mIYWuNOXn48X7d3YjKqIWU7/cytoDpxxyzLSsHL6PSWRoZEOCAz2jk7igEVFhbDp8mqNn0l1y/Nw8ww/bj3F9m3oe01GvinAuEQ6shM63gVfFDBvXROBG4lPSSDidfln/QEGBfj58PLEHTUMDmfzpJrYlnCn3cRfGHiM1M4cJvTynk7igi/MIFrqo0zg6Lpmk85mMcINhu8rFYmeBybMSQQXRROBGLs4fuKpF0YkAoFagH5/f24vaQX5M/Hgj+0+mluu4szYeoWW96nRv4vxOKXfVNDSIqEbBzHdRIpgfm0iQnzfXt3VOUTFVSRgDW2dCk6shpOLqKWkicCPr41KoHehLazsWia9fM4Av7u2Flwh3fRRd5iaNPcfPsfXIGY/sJC5oeFQYOxPPcSCpfIm1tLJy8li84zg3dGhQadYfVk5yZD2kHKiwTuKLNBG4CWMM6+OS6d28TqH9A4VpGhrEZ5N6cj4zhzs/iia5DMMfZ2+Ix8/bi9FdPa+TuKBhncIQocI7jX/+zZrZrKOFFDFfgG8QtB9ZoYfVROAmEk6nc/RM8f0DhWkfVpOP7u7B0dPpTPx4I6mZOUVvnJcHWz6Ds0cBq8Lpd1sSGNyxQaVcDN3RGgQH0LNpCAu2JTp9Fnd+C7YlUjvQt9CFWDhzBDZ+CDlZFRaPxzh/ArZ9bTXHuIOsC7BzHnQcBf4Vu/6yJgI3cWn+QAn9A4Xp2SyEaXd0Zfexc0z+dBMZ2bmFbxj7JcyfCh8OgGPbWLT9GOcychjf00MW+LDDiM5hxCVdYGfiuQo5XnpWLst2neCmyIZXLqhzdAt8MAB++BN8eQtkVExMHsEYmDcFvrsP9ix0dTSWXd9DVip0vqPCD62JwE2sj0umTpAfreqV7Urg+rb1ef2WKNbFJfPHWVuvXAA+8zyseBHqR1pL3n18E7vXzKVpncDLJq95uiEdG+LjJRVWcmL57hOkZeVeuS7xb0vhk6HgEwAD/g6HfoGPb7p0N6fKad8ya4imTwD8+Kw1k9fVts6EkBbQuHeFH1oTgRswxrDO1j9Qng7bm7uE8/zw9vy46wR//a5Akbpf/metcjT8DbhvGZk1Ingy5e88F7HV4zuJ86sd5Ee/VqEsiE0krwIqvs6PTaR+TX96Ngv5/clNH8Os8RDaCu5bDv3+BLfNgdOH4cOBcGKn0+Oq0nKzYenT1kn31s/g9CGIfs+1MaXEweFfrCGjLvh71ETgBo6kpHHsbAa9y9AsVNDEq5vxyIBWfL05gX8u3mMlgzNHYO07EHkrNOoONcN4s/HbrDcduG7PC7Dqn+7TTuoGhkeFkXg2gy1HTjv1OGfTs1m9N4lhncKsUtt5ebD8BVj4KLQcCBMXQQ3bLOOWA2DSYsDAjMEQ95NTY6vSNs2A5H1w4yvWql+tboSfX4fUJNfFFPMliBdETXDJ4TURuIHf6wuFlLClfR4d2Iq7r2rC9J/jmLb6ACx7zvqQDXwOgMycXGbFnmZ26/9Y7ZGrX4XvH7aulBQ3dGiAv4+X0+cULN15nKzcPGu0UE4WzP0D/PJf6Ho3jJ91ZYdhg0jrDiG4EXwxxlrQXJVOWgqs+odVuqH1YOu5G16G7DRY9YprYsrLhZhZ0OJ6CA53SQiaCNzAurhkQqv706KuY0YKiAjPDe/AyM5hrFg6H3Z+B1c/Yp1AgKU7T3A6LZtbezaHke9A/79aw9ZmaockQHV/Hwa0q8ei7ceu7GtxoAWxiTSpE0inUANfjLZWo7r+bzD8TfAuYvHA4EZwz2Jo0sdKHD+/pndzpbH6X5B5Dm78x+9NMHVbQ4/JsOVTOL6j4mM6uBrOJVT43IH8NBG42O/zB0Ic2lbv5SW8PjaS12vM4pgJYWmtWy+9Niv6CI1qV6Nvy1Drj6H/UzDyXTi0Bj4eYtU68XAjosI4lZrFWtvdmqMlnc/k1/2nuK2NFzLjJmsi0ajpcM0TJbcRV6sFt38LncbBypdhwR/1bs4eSXthwwfQbSLU73D5a9f+BQKCrb6Dik6sW2dCQC1oM6Rij5uPJgIXO3jqAifOZZZp2GhJfHd8TbOs3/g6eBIPf7OXNfuSOHTqAuvikhnfI+LyiWtd7rB1SB60dUjucng8lUn/NvWo7u/jtNFDi3ccoy2HuHfvZDh3FO74FqLG2b8DHz8Y9T70e8KaGzJrPGRW7IzoSufHZ8EvCK575srXAkOsO+ODq2Hv4oqLKf007F4AkbeAb0DFHbcATQQutj4uBaDUE8lKlHUBVrwAYV25e8pfaFG3On/4fDMvLdyFt5dwS/dC5g60HGA1O+Tl2jokVzs2pkokwNebGzrUZ8nO42TmFDEvoxwORy/gG/+X8PH2gUlLofm1pd+JCAz4Gwx7Aw6sgk+GwHn3WHLT7exfDvt+hGv+DEGFTNwD6D4JQlvbhpNW0AS+Hd9CbiZ0cV2zEDg5EYjIYBHZKyL7ReSpQl4PFpEFIhIrIjtF5B5nxuOO1sUlU6+GP81Dgxy741/fhPPHYPCrBAf689m9Palbw58Ve05yfdt61K9ZxNVHw05Wh2TNMFuH5FeOjasSGREVxvmMHH7a69jRJKd/ncFTp58jLaiR9bsu78Lk3e+BCbPh1H74cJDVBKJ+l5sDS5+B2s2g1x+K3s7b1+o7SDkAGz+omNi2zoT6HaFh54o5XhGclghExBt4F7gJaA9MEJGCn/iHgF3GmCigP/AfEfGYWgf56ws5dCz/mXgrEXQcA417AVCvhlWkrm/LUB6+rmXx768VAZOWWBNb5t5vDa3zwA7Jq1uGEhLk57jRQ8bAqn9Se9ljrMtrT/odP1gJ1xFa3wD3/AA5GfDRIDj0q2P2WxVs/hiS9lijg3xKWAK01SBr6O5P/4ILzukfuuTkbkjcYnUSu3gujzPvCHoC+40xccaYLGA2ULCSkgFqiHUWrA6kAMUUy6laDiRdIOm8E/oHVrxgfR34/GVPR4QE8sV9vYiyZ03iarWsduvIW2HlS9bY9lyP+a8BwNfbi5s6NmDF7hNcKK6Gkz1ys60huqtf5Ue/gbxZ/xUiGjp4JbKwLtYdRvX68PnNsP0bx+6/Mko/bQ0XbdoP2g617z03vGKVevjpH86NbesX4OUDnW4teVsnc2YiCAfyL66bYHsuv3eAdkAisB14xBhzxXg9EblfRDaJyKakJBdO+nCwS+sPOLJ/IH6Dteh1n6lQq5wLzfj4w+jp1szWzZ/A7Ake1yE5IiqMjOw8lu8+UfadZJyzhubGfEFKjz9x/7l7GNrZSYsA1W4C9/4IjXrAt/fCL2945N3cJatfs5JB/uGiJanX1uov2DTDump3htxs2PaVNZehqD6LCuTMRFDYb73gJ/JGIAYIAzoD74hIzSveZMx0Y0x3Y0z3unXrOjpOl1kXl0yDmgE0qRPomB3m5cGSv0L1BnD1o47Zp4hV62bYG1aH2ydDraqNHqJH0xAaBgeUvTT1uURrSO6hNTDyXT71G4+IMLRTQ8cGml+12nDHd9BhNCx/DhY9YQ0A8DSn9sOG96HrnVbfV2n0/yv413DecNJ9P8KFJOhyp+P3XQbOTAQJQP6hKY2wrvzzuwf4zlj2AweBtk6MyW0YY4iOS+aqFg7sH9jxLRzdZJ24HV3G9lKH5G/W8FIP6ZD08hKGdWrIz/uSOJNWypEkJ3ZZnbenD8JtczCdb2dBbCK9m9UpurPeUXwDYMxH1kTCjR/CV3dAVppzj+lulv0NfKpZk/RKK6gOXPuUVZhu3zLHx7Z1ptWE13Kg4/ddBs5MBBuBViLSzNYBPB6YX2CbI8AAABGpD7QB4pwYk9vYfzKVU6lZ9HZQWQmy0qyrv4ZRzqtX0vpGmPgD5KTDRzfA4bXOOY6bGR4VRnauYcmOUgzNjFttDcHNy7GG5LYcwM7Ec8SdulBx6xJ7ecGgF2HI6/DbEvh0mGvr6VSkA6tg7yK45k9QvYzLf/a4D+q0tO4KHDlhLzUJ9i21JgQWNYO8gjktERhjcoCHgaXAbmCOMWaniEwRkSm2zV4C+ojIdmAF8KQx5pSzYnIn6y71DziofXDt29bEpMGvWicAZwnvanVIBtWFz0ZadyFVXGR4ME3rBNo/eij2K2vobc0w63dla5ZYEJuIj5cwuEMDJ0ZbiJ6TYdwX1h3KRwOtJpOqLDfHOnnXagK9Hij7fnz8rI7j5H2w8SPHxbftK+sCoUvFrztQFKfOIzDGLDLGtDbGtDDGvGJ77j1jzHu27xONMTcYYyKNMR2NMV84Mx53sj4umbDgACJCqpV/Z2ePwq9vQPubrRo0zla7qdUhGd4NvpkEv75VpTskRYQRUWGsi0vm5LmMojc0xhpqO/d+a+jtpCXWUFwgL8+wIDaRa1rXpbYrVoNrOxQmLrQ6+z8aBEeiKz6GirL1Mzi5C254qfyzdVvfCM2vg5/+aRWsKy9jrNFC4d2hbpvy789B3OO+xMPk5RnWx6XQv01dx/QPrHjRusIY9EL592WvwBC4c561ytOyv1mlrm/6F3hVzcXXh0eF8dbK/SzcdoxJfZtduUFujtUpu/lja8jtyHcuG7O++chpEs9m8JfBLuwCa9Qd7lsGX4yFT4dbo8ECrhibUU5inTxDCvkdVYSMs7DyFWhyNbQbUf79iVgjjt672ipYd9O/yre/xC2QtNsafOFGNBG4wL6TqaRcyHJMWYmjm2HbbOj7mHWlXpF8A2DMDKgZDuvesUbIjPkQ/Bw0CsqNtKpfg7YNarBgW+KViSAzFb65xxoJ0vdxq7O+QIJfEJtIgK8Xg9o7eO5AaYU0h3uXwVe3O2+c/E//hAmzKubutKCfX4e0ZGutAUcNwqjf3ipUt+EDa1hpea7kt860OrA7jnZMbA6iicAF1h2wukHKPX/AGGu4aFA96wTkCl5e1h9drSaw+C/WleZtX7nF2GhHG9E5jH8v2Ut8ShoRIbZkd/4EfHkrHN9mXeV1v7JKSk5uHou2H2NA2/oE+bvBn1xQHasDO+OM4/edehJm3271H416v2JPeMkHYP00a6ZuWBfH7vu6Z6wJej8+C7d/XbZ9ZGfAjm+g3XCr0qkb0aJzLrA+LoVGtav9fjIpq53fQXy0VXjM4bf4pdTrfluH5A5reGnyAdfG4wQX1xW+1Gmc9Jut8/U3a2htIUkAYO2BZE6lZjE8qoJGC9lDxJpv4Oh/ddvk6z+6p2L7j5b9Hbz9rL8HRwsKtUpV7/sR9i0v2z72LLSarlxcYK4wmggqWF6eYf3B5PI3C2WnWyuPNYh06YIWl2k3DO5eaC388eFAa5ZzFRIREkjXxrWs0tSH11qdrtkZ1pDa1jcW+b75sYnU8Pehf5uqMxmyWBf7jzqMsvqPFv/F+RPaDv5snWj7PQ41nDQqq+cfrKa1pU+XrdzK1i8guDE0vcbxsZWTJoIKtvfEec6kZZe/WWjdO3A2Hm78p3t10Eb0sNqgq9Wymol2L3B1RA41PCqMFieXkffpSGsI7X3LrCG1RcjMyWXpjuPc2LEBAb5u9P/kbBf7j/pMhQ3T4as7nTehLS8XljwNwRFw1UPOOQZYw0kHvQSn9lqDAkrjTLy1znTn25w7vLuM3C+iKu7i+sTlWqj+3DFY8z9oOwya9XNQZA5Up4WVDBpEWieA9e+5OiLHMIaxmXN51+8tEoPaWU0gJXTQ/7Q3ifOZOe7VLFRRvLysip83vWZN7vp0OFxwwjShmJlwYrs1as7XAcOxi9N2qFXAbtU/rBpG9oqdDRjo7JrF6UuiiaCCrY9LpnFIIOG1yvGBXfky5GZZ46TdVVAo3DXf+sNZ8qR1xZbnvPV/nS4vFxb/hRo/v8D6atdwT+4zmGq1S3zb/NhE6gT5cbUTVqCrNJzZf5RxDla8BBG9rNpKziYCg/9pJYHVr9n3HmOsZNW0X8WP7LOTJoIKlJdniD6YUr6yEolbrQ9V7wes9kp35hcIt34GvabA+nfhm4lWm3plk5Vm3dlsmA59pnL4urfZl5LDtoSzxb7tQmYOK3afYEhkQ3y8PfxPzVn9R7/8Fy6ctE7OFVXTv0EkdL3LKmhnzyztw2utelNuNJO4IA//dFas3cfPcTY9u+zrDxhjXVkH1rEWOa8MvLytshc3vAK7vreGFTpihmZFuXDKatLYu8hq4rjhZQZ3DMfXW0pcz3j57hNkZOdVXG0hd+fo/qPTh2Ddu1ZtrfBujojQftc/a80H+PHZkrfd+gX41XDMBDcn0URQgS71D5S1o3jX93BkLVz/jNuNQy6WCPR5GG75xLqj+WgQpBx0dVQlSz5gXb2e2GE1bfS6H4DgQF+ubV2XhduOkZdX9NDI+TGJNAwOoFvjkpuQPIYj+4+W/d1a2GXA3x0Xn72q17MK2v222CpwV5TM87BrnjWfwo0nWmoiqEDr45JpWieQhsFl6B/IzrCG4tXrAF3ucnxwFaHDKLjre2vm50eDrFnR7ip+gxVj5jmrSaPdsMteHh4VxvFzGWw4VPjdzZm0LH7el8TwqDC8vFy7DKHbKdh/tPSZ0vcfHV5rXRhd/ajjlvssrV4PWBMpixtOunMeZKe5dbMQaCKoMLmX+gfKeDcQPc2q5zP4H25TurZMmlxlXRH6BsInw2DvYldHdKXdC6ymi4BgK9aIHldsMqh9far5ehdZkXTxjuNk5xpGeOJoIXtc7D/q+QdrKHRp+o/y8mDJU1Zpkz5TnRpmsXwDrAEbJ3dZhe4KEzMT6rSyVoxzY5oIKsiuxHOcz8gpW//A+RPw83+gzRBo3t/hsVW40FZWeea6bWD2bdbCKe5i/XtWk0WDSCsJ1GlR6GaBfj4MbF+fxduPkZ175dXsgthEmocG0SHMxTO+3ZmXt1XErbT9R7Gz4FgsDHzB9c0t7UZYBe5WvmzNGs7v1H44ss6aSezixelLoomgglxcn7hMdwSrXoacDGtMdlVRvZ41I7fVDfDDn6xZ0q4cXpqXZzVRLHnSarK4a36J9ZKGd2rI6bRsftl/+dj4k+cyWBeXzLCoMMetPldVXdF/dEPx/UeZqbDiBesKO3JshYVZpIvVSdNS4OcCw0ljZoJ4O2+hKAfSRFBB1sUl0zw0qPRLFB7bBls+h573F3l1Wmn5BcG4mVZFx1/fgO8mQ05mxceRnWHVxVn3jtVUcetndl1pXtumLjUDfFhQYD3jhduOYQzaLFQal/qPThXff/TL/yD1hDWj3l2SbFhnq8zL+vd+nyORl2tNIms50HklLxyoxEQgIsNERBNGOeTk5rHxYErpZxMbY3VEVasN1/7ZOcG5mrcPDP0vDHjOqsz4+ejSzdgsr7QUq0li1zyriaIUayr4+3gzuGMDlu48Tkb277V05scm0r5hTVrWc/C60VXdpf6jarb+oyWXv37miJWsI28ptN/GpQb8zSp4t8w2gunAKjif6JYF5gpjzwl+PLBPRP4tIu2cHVBVtDPxHOczc0rfLLTnBzi0Bq6zJYOqSsQqFjb6Q6ua6ozB1h+9s6UctJoiErdaTRN9Hi71VeaIqHAuZOWyas9JAOJT0oiJP6NzB8oqtBXct8LWfzTh8iUilz8PCAx83kXBFaNGA+szvGehVQAv5guoFgKtb3J1ZHYpMREYY+4AugAHgI9FZJ2I3C8iNZweXRXxe/9AKWYU52Rak1XqtoVuhZc3rnI63QJ3zrVqKX040OoQdJajm60miLRTVpNEh1Fl2k3v5iGEVve7NHro4tdhnRo6LFSPc1n/0eNWAjiy3lof++o/QnAjV0dYuKsesqqLLvqzdRHX6VarUF0lYNc4RGPMORH5FqgGPAqMAv4sIm8ZY952YnxVwrq4ZFrUDaJejVL0D0S/Z01Lv+O7yj1ctLSa9YN7l1rLKX48xGoyCixHSY7CXDhldTgGhVq/39BWZd6Vj7cXQyMbMmtjPOczslkQm0i3JrVpVNt9Jw9VChf7jxb/2eoXiH4fajSEqx9xdWRF861mFb77xnbh5uZzB/Ir8QwjIsOBSUAL4HOgpzHmpIgEArsBTQTFuNg/MKpruP1v2jrTWoe49WBoOcB5wbmreu2s4aVf3mqdCJwhrAvcNse6+iynEZ3D+HTdYd5ZtZ89x8/zwogODghQXeo/Co6w/h5GvG0lCHfWYRRsmmGN8msQ6epo7GbPpeYtwP+MMT/nf9IYkyYik5wTVtWx/ehZLmTl2tc/YIy1QPZP/7TmC4z+wOnxua2aDWHyKuuuyBlqN3PYnVbXxrUJr1WN6T/H4SUwJFKbhRzmYv9Rz8ngXwlao0Xg9m/AVK5Ku/b8JTwHHLv4QESqAfWNMYeMMSucFlkVsT7OmiBTYiLIzYaFj1oFqqJug+FvVpr2Rafx9ilXs01FERGGRTXk/dVxXN0ylLo1/F0dUtVTGZLARb6lHCLuBuwZNfQ1kD+95dqeU3ZYF5dMq3rVCa1ezMkh45zVDLL1C7j2Sbj5/zQJVDKjuoTjJTCmWymaAJVyE/bcEfgYY7IuPjDGZImInqXskJ2bx6ZDKYztVswoh3PHYOYtVr2SEW9bdc5VpdO2QU3WPHk9YcGV72pQKXvuCJJE5FIhbREZCThhvbmqZ1vCWdKycoten/jkbmuY5OmDcPscTQKVXHitalpSQlVK9twRTAFmisg7gADxgJ6x7HBx/kCvwhLBwZ9h9h3WkLN7FkHDqAqOTimlLCUmAmPMAaC3iFQHxBhz3vlhVQ3r45Jp26AGIUEFWtK2fQ3zHrBqB93+NdRq7JoAlVIKOyeUichQoAMQcPHW1xjzohPjqvSycvLYdOg043pE/P6kMdYaqytetBayHvd51S4doZSqFOyZUPYeEAhcB3wIjAUctPJ01bUt4Qzp2fnmD+TmWJOjNs2wimaNfBd8dJihUsr17Oks7mOMuQs4bYx5AbgKiCjhPR5v3YFkRKBXsxCrhvrs26wk0PcxGDVdk4BSym3Y0zR0cf24NBEJA5KBZs4LqWpYfzCZtg1qUtucgU9ugePbrOnyPe51dWhKKXUZexLBAhGpBbwGbAEM4MG1D0qWmZPLpkOnmRqVBx8OsIqcjZ8FbQa7OjSllLpCsYnAtiDNCmPMGeBbEVkIBBhjzhb3Pk8XG3+WyNxd/OG3N8HPDyYuhPBurg5LKaUKVWwfgTEmD/hPvseZpUkCIjJYRPaKyH4ReaqQ1/8sIjG2fztEJFdEHFxzuOIlRX/FTL9/4lU91FpxSZOAUsqN2dNZ/KOIjJFSTpkUEW/gXeAmoD0wQUTa59/GGPOaMaazMaYz8FdgtTEmpTTHcSvGwNp3GLrnKfb7tsJ78nII0e4UpZR7s6eP4HEgCMgRkQys2cXGGFOzhPf1BPYbY+IARGQ2MBLYVcT2E4BZdkXtjvJyrfWFo99jUV5vYju9SgdHL6iilFJOYM/M4rLWfw3HKkdxUQLQq7ANbYvcDAYeLuOxXCsrDb6bDHsWktjuXh7aeh0ftNQ1a5VSlYM9E8quKez5ggvVFPbWwt5WxLbDgV+LahYSkfuB+wEaNy5bOYbEM+n8Y9FunhvewbH14i8kw6xxkLAJBv+Lr85fj8g+ejTTuwGlVOVgT9NQ/rUCA7CafDYD15fwvgQun3jWCEgsYtvxFNMsZIyZDkwH6N69e1HJpFg7jp5l2a4TrI9L5vVboujfpvxLFAKw8kU4ts0qF9FuOOveX0eHsGCCq/k6Zv9KKeVkJXYWG2OG5/s3COgInLBj3xuBViLSzLZ+wXhgfsGNRCQYuBb4vnShl84NHRow/+G+1AnyZ+LHG3lxwS4yc3LLv+NDv0KL66HdcDKyc4k5coarWtixLKVSSrkJe0YNFZSAlQyKZYzJwWrzX4q1yP0cY8xOEZkiIlPybToK+NEYc6EMsZRKmwY1+P7hq7n7qibM+PUgN7+7lv0ny1FM9UIyJO+DiJ4AbDl8mqzcPHo312YhpVTlYU8fwdv83rbvBXQGYu3ZuTFmEbCowHPvFXj8CfCJPftzhABfb14Y2ZF+rery529iGfb2Lzw3vAPje0SUflGRBFvtvca9AavstJdAj6aaCJRSlYc9dwSbsPoENgPrgCeNMXc4NaoKMLB9fZY8eg3dm4Tw1++28+DMLZxJyyr5jfnFR4OXD4R1Aaz1iSPDg6kRoP0DSqnKw57O4m+ADGNMLlgTxUQk0BiT5tzQnK9+zQA+m9STD9bE8drSvcTEn+GNcZ0LX1GsMEeirZXFfKuRnpVLTPwZJvXVCWRKqcrFnjuCFUC1fI+rAcudE07F8/IS/nBtC757sA/+Pl5M+GA9//lxLzm5ecW/MScLErdAhDU1YvPh02TnmqLXJ1ZKKTdlTyIIMMakXnxg+z7QeSG5RqdGtVj4x36M7tqIt1fu59b31xGfUsxNz/HtkJNxKRGsj0vG20vorv0DSqlKxp5EcEFEul58ICLdgHTnheQ61f19eP2WKN6a0IV9J1IZ8uYavo85WvjG8eutr7ZEsC4umU6Ngqnub9fqn0op5TbsSQSPAl+LyBoRWQN8RWUtBWGnEVFhLHqkH63qV+eR2TH8aU4sqZk5l28UHw3BjaFmQy5k5hAbf+b3ZSmVUqoSsafW0EYRaQu0wSobsccYk+30yFwsIiSQOX+4irdW7OOdVfvZfDiFN8d3ISqillVlNH4DNO0LWP0DOXnaP6CUqpxKvCMQkYeAIGPMDmPMdqC6iDzo/NBcz8fbi8dvaMOsyb3JysljzLS1vLf6AHmnD8P5Y5c1C/l4Cd2a1HZxxEopVXr2NA1Ntq1QBoAx5jQw2WkRuaFezeuw+JFrGNS+Pq8u3sO0L760XrDNKF4fl0xURC2CtH9AKVUJ2ZMIvPIvSmNbcMbPeSG5p+BAX/7v9q68OjqSWqdiuEAAK5JDSc3MYVvCWS0roZSqtOy5hF0KzBGR97BKTUwBFjs1KjclIozv2ZjMDQnsOtOGe7+IoU+LOuTmGa5qHurq8JRSqkzsuSN4EmtS2QPAQ8A2Lp9g5lkyz+OfvJvI3oO4t28z1h5Ixtdb+weUUpWXPaOG8kRkPdAcGAeEAN86OzC3dXQzmDx8ml7F31q25/q29TidlkU1P29XR6aUUmVSZCIQkdZYawhMAJKx5g9gjLmuYkJzU0eiAYHw7gBc3VKbhJRSlVtxdwR7gDXAcGPMfgAReaxConJn8dFQrx1Uq+XqSJRSyiGK6yMYAxwHVonIByIygMLXIfYceXmQsPHS/AGllKoKikwExpi5xphxQFvgJ+AxoL6ITBORGyooPveStBsyz2kiUEpVKfasWXzBGDPTGDMMawH6GOApZwfmluKjra+2iWRKKVUVlGrNYmNMijHmfWPM9c4KyK3Fb4CguhDS3NWRKKWUw5Rl8XrPFR9tNQuVdm1jpZRyY5oI7JV6ElLitFlIKVXlaCKwV/wG62tEb9fGoZRSDqaJwF7x0eDtZy1Wr5RSVYgmAnvFR0PDzuAb4OpIlFLKoTQR2CMnExK3av+AUqpK0kRgj2OxkJsFjbV/QClV9WgisMeR9dbXRnpHoJSqejQR2CM+Gmo3hRr1XR2JUko5nCaCkhhjDR3VYaNKqSpKE0FJTh+ECye1o1gpVWVpIijJpYlkWnFUKVU1aSIoSXw0+Ne0FqNRSqkqSBNBSY5EQ6Pu4KVrEiulqiZNBMXJOAsnd2mzkFKqStNEUJyETYDRRKCUqtKcmghEZLCI7BWR/SJS6KpmItJfRGJEZKeIrHZmPKUWHw3iBeHdXB2JUko5jY+zdiwi3sC7wCAgAdgoIvONMbvybVML+D9gsDHmiIjUc1Y8ZRIfDfU6QEBNV0eilFJO48w7gp7AfmNMnDEmC5gNjCywzW3Ad8aYIwDGmJNOjKd08nKtpqHG2iyklKranJkIwoH4fI8TbM/l1xqoLSI/ichmEbmrsB2JyP0isklENiUlJTkp3AJO7ISsVO0fUEpVec5MBIUt7GsKPPYBugFDgRuBv4lI6yveZMx0Y0x3Y0z3unXrOj7SwsRHW191RrFSqopzWh8B1h1ARL7HjYDEQrY5ZYy5AFwQkZ+BKOA3J8Zln/gNUL0B1Gri6kiUUsqpnHlHsBFoJSLNRMQPGA/ML7DN90A/EfERkUCgF7DbiTHZL369dTcghd3YKKVU1eG0RGCMyQEeBpZindznGGN2isgUEZli22Y3sATYBmwAPjTG7HBWTHY7dwzOHNH+AaWUR3Bm0xDGmEXAogLPvVfg8WvAa86Mo9QSbIXmdEUypZQH0JnFhYnfAN7+0KCTqyNRSimn00RQmCPrIbwr+Pi5OhKllHI6TQQFZadbi9XrsFGllIfQRFBQYgzkZevSlEopj6GJoKD49dZXvSNQSnkITQQFxW+AkBYQFOrqSJRSqkJoIsjPGKu0hA4bVUp5EE0E+SUfgLRkbRZSSnkUTQT5XSo0pzOKlVKeQxNBfvHREBAMoW1cHYlSSlUYTQT5xUdDo57gpb8WpZTn0DPeRemnIWmPNgsppTyOJoKLEjZZX3VpSqWUh9FEcNGR9SDeENbV1ZEopVSF0kRwUXw0NOgI/tVdHYlSSlUoTQQAuTlwdLPWF1JKeSRNBAAntkN2mk4kU0p5JE0EYNUXAh0xpJTySJoIwOofqBkOtSJcHYlSSlU4TQQAR6K1WUgp5bGcunh9pXA2Ac4lQMTDro5EqVLLzs4mISGBjIwMV4ei3ERAQACNGjXC19fX7vdoItD+AVWJJSQkUKNGDZo2bYqIuDoc5WLGGJKTk0lISKBZs2Z2v0+bhuI3gE81aBDp6kiUKrWMjAzq1KmjSUABICLUqVOn1HeImgji10N4N/C2/zZKKXeiSUDlV5bPg2cngqwLcGybdhQrpTyaZyeCxK1gcnVpSqXKae7cuYgIe/bscXUoqgw8OxEcWW99bdTDtXEoVcnNmjWLvn37Mnv2bKcdIzc312n79nSePWoofgOEtobAEFdHolS5vbBgJ7sSzzl0n+3DavLc8A7FbpOamsqvv/7KqlWrGDFiBM8//zy5ubk8+eSTLF26FBFh8uTJTJ06lY0bN/LII49w4cIF/P39WbFiBd9++y2bNm3inXfeAWDYsGE88cQT9O/fn+rVq/P444+zdOlS/vOf/7By5UoWLFhAeno6ffr04f3330dE2L9/P1OmTCEpKQlvb2++/vprnn/+ecaOHcvIkSMBuP322xk3bhwjRoxw6O+oKvDcO4K8PEjYoMNGlSqnefPmMXjwYFq3bk1ISAhbtmxh+vTpHDx4kK1bt7Jt2zZuv/12srKyGDduHG+++SaxsbEsX76catWqFbvvCxcu0LFjR6Kjo+nbty8PP/wwGzduZMeOHaSnp7Nw4ULAOsk/9NBDxMbGsnbtWho2bMh9993Hxx9/DMDZs2dZu3YtQ4YMcfrvozLy3DuC5H3WqmSaCFQVUdKVu7PMmjWLRx99FIDx48cza9Ys4uLimDJlCj4+1ikmJCSE7du307BhQ3r0sJpia9asWeK+vb29GTNmzKXHq1at4t///jdpaWmkpKTQoUMH+vfvz9GjRxk1ahRgTagCuPbaa3nooYc4efIk3333HWPGjLkUj7qc5/5W4qOtr5oIlCqz5ORkVq5cyY4dOxARcnNzERG6det2xTBGY0yhQxt9fHzIy8u79Dj/GPiAgAC8vb0vPf/ggw+yadMmIiIieP7558nIyMAYU2R8d955JzNnzmT27NnMmDGjvD9uleW5TUPx0VCtNoS2cnUkSlVa33zzDXfddReHDx/m0KFDxMfH06xZM7p27cp7771HTk4OACkpKbRt25bExEQ2btwIwPnz58nJyaFp06bExMSQl5dHfHw8GzZsKPRYFxNEaGgoqampfPPNN4B1Z9GoUSPmzZsHQGZmJmlpaQBMnDiRN954A4AOHVxzx1QZeG4iOBJt3Q3oZBylymzWrFmXmmQuGjNmDImJiTRu3JhOnToRFRXFl19+iZ+fH1999RVTp04lKiqKQYMGkZGRwdVXX02zZs2IjIzkiSeeoGvXwpeLrVWrFpMnTyYyMpKbb775UhMTwOeff85bb71Fp06d6NOnD8ePHwegfv36tGvXjnvuucd5v4QqQIq7rXJH3bt3N5s2bSrfTi4kw2vNYcDfod+fHBOYUi6we/du2rVr5+ow3FZaWhqRkZFs2bKF4OBgV4dTYQr7XIjIZmNM98K298w7ggTr1lSXplSq6lq+fDlt27Zl6tSpHpUEysKpncUiMhh4E/AGPjTGvFrg9f7A98BB21PfGWNedGZMgFVfyMsHwro4/VBKKdcYOHAgR44ccXUYlYLTEoGIeAPvAoOABGCjiMw3xuwqsOkaY8wwZ8VRqPgN0KAT+AVW6GGVUsodObNpqCew3xgTZ4zJAmYDI514PPvkZsPRzVpfSCmlbJyZCMKB+HyPE2zPFXSViMSKyGIRKXR8l4jcLyKbRGRTUlJS+aI6tg1yMrTiqFJK2TgzERQ2LrPgEKUtQBNjTBTwNjCvsB0ZY6YbY7obY7rXrVu3fFHpRDKllLqMMxNBAhCR73EjIDH/BsaYc8aYVNv3iwBfEQl1YkxWIghuDDXDnHoYpTxB//79Wbp06WXPvfHGGzz44IPFvufiEPAhQ4Zw5syZK7Z5/vnnef3114s99rx589i16/cux7///e8sX768FNEX75FHHiE8PPyyWc9VlTMTwUaglYg0ExE/YDwwP/8GItJAbHPORaSnLZ5kp0VkjJUItFlIKYeYMGHCFaWnZ8+ezYQJE+x6/6JFi6hVq1aZjl0wEbz44osMHDiwTPsqKC8vj7lz5xIREcHPP//skH0Wxl1Kaztt1JAxJkdEHgaWYg0fnWGM2SkiU2yvvweMBR4QkRwgHRhvnDnD7Ww8nD+mzUKqalr8FBzf7th9NoiEm14t8uWxY8fy7LPPkpmZib+/P4cOHSIxMZG+ffvywAMPsHHjRtLT0xk7diwvvPDCFe9v2rQpmzZtIjQ0lFdeeYXPPvuMiIgI6tatS7du3QD44IMPmD59OllZWbRs2ZLPP/+cmJgY5s+fz+rVq3n55Zf59ttveemllxg2bBhjx45lxYoVPPHEE+Tk5NCjRw+mTZuGv78/TZs25e6772bBggVkZ2fz9ddf07Zt2yviWrVqFR07dmTcuHHMmjWL/v37A3DixAmmTJlCXFwcANOmTaNPnz589tlnvP7664gInTp14vPPP2fixImX4gGoXr06qamp/PTTT7zwwgs0bNiQmJgYdu3axc0330x8fDwZGRk88sgj3H///QAsWbKEp59+mtzcXEJDQ1m2bBlt2rRh7dq11K1bl7y8PFq3bs369esJDS17Y4pT5xHYmnsWFXjuvXzfvwO848wYLhNvq2HSWBOBUo5Qp04devbsyZIlSxg5ciSzZ89m3LhxiAivvPIKISEh5ObmMmDAALZt20anTp0K3c/mzZuZPXs2W7duJScnh65du15KBKNHj2by5MkAPPvss3z00UdMnTqVESNGXHaivSgjI4OJEyeyYsUKWrduzV133cW0adMuVUgNDQ1ly5Yt/N///R+vv/46H3744RXxzJo1iwkTJjBy5EiefvppsrOz8fX15Y9//CPXXnstc+fOJTc3l9TUVHbu3Mkrr7zCr7/+SmhoKCkpKSX+3jZs2MCOHTto1qwZADNmzCAkJIT09HR69OjBmDFjyMvLY/Lkyfz88880a9aMlJQUvLy8uOOOO5g5cyaPPvooy5cvJyoqqlxJADyt+uiR9eAbBPW0+JSqgoq5cnemi81DFxPBxSqfc+bMYfr06eTk5HDs2DF27dpVZCJYs2YNo0aNIjDQmtuTf/GYHTt28Oyzz3LmzBlSU1O58cYbi41n7969NGvWjNatWwNw99138+67715KBKNHjwagW7dufPfdd1e8Pysri0WLFvG///2PGjVq0KtXL3788UeGDh3KypUr+eyzzwCrRHZwcDCfffYZY8eOvXQyDgkpeaGrnj17XkoCAG+99RZz584FID4+nn379pGUlMQ111xzabuL+500aRIjR47k0UcfZcaMGQ6po+RZiSA+Ghp1A2/P+rGVcqabb76Zxx9/nC1btpCenk7Xrl05ePAgr7/+Ohs3bqR27dpMnDjxsvLShSmsRDVYFUTnzZtHVFQUn3zyCT/99FOx+ympddnf3x+wTuQXq6Pmt2TJEs6ePUtkZCRg1SsKDAxk6NChRR6vpPLaxhiysrIuvRYUFHTp+59++only5ezbt06AgMD6d+//6Xy2oXtNyIigvr167Ny5Uqio6OZOXNmsT+vPTyn1lBmKpzYof0DSjlY9erV6d+/P5MmTbrUSXzu3DmCgoIIDg7mxIkTLF68uNh9XHPNNcydO5f09HTOnz/PggULLr12/vx5GjZsSHZ29mUnvRo1anD+/Pkr9tW2bVsOHTrE/v37Aasy6bXXXmv3zzNr1iw+/PBDDh06xKFDhzh48CA//vgjaWlpDBgwgGnTpgFWR++5c+cYMGAAc+bMITnZGudysWmoadOmbN68GYDvv/+e7OzsQo939uxZateuTWBgIHv27GH9emst9auuuorVq1dz8ODBy/YLcN9993HHHXdw6623XlqvoTw8JxEc3QwmTwvNKeUEEyZMIDY2lvHjxwMQFRVFly5d6NChA5MmTeLqq68u9v1du3Zl3LhxdO7cmTFjxtCvX79Lr7300kv06tWLQYMGXdaxO378eF577TW6dOnCgQMHLj0fEBDAxx9/zC233EJkZCReXl5MmTLFrp8jLS2NpUuXXnb1HxQURN++fVmwYAFvvvkmq1atIjIykm7durFz5046dOjAM888w7XXXktUVBSPP/44AJMnT2b16tX07NmT6Ojoy+4C8hs8eDA5OTl06tSJv/3tb/TubZ2j6taty/Tp0xk9ejRRUVGMGzfu0ntGjBhBamqqw8pre04Z6iPrYc1/YfR0qFbL4XEp5Qpahtozbdq0iccee4w1a9YU+nppy1B7TmN5495w+xxXR6GUUuXy6quvMm3aNIf0DVzkOU1DSilVBTz11FMcPnyYvn37OmyfmgiUquQqW/Oucq6yfB40EShViQUEBJCcnKzJQAFWEkhOTiYgIKBU7/OcPgKlqqBGjRqRkJBAucuzqyojICCARo0aleo9mgiUqsR8fX0vm6GqVFlo05BSSnk4TQRKKeXhNBEopZSHq3Qzi0UkCThcxreHAqccGI6zVaZ4K1OsULnirUyxQuWKtzLFCuWLt4kxptC1fitdIigPEdlU1BRrd1SZ4q1MsULlircyxQqVK97KFCs4L15tGlJKKQ+niUAppTycpyWC6a4OoJQqU7yVKVaoXPFWplihcsVbmWIFJ8XrUX0ESimlruRpdwRKKaUK0ESglFIezmMSgYgMFpG9IrJfRJ5ydTxFEZEIEVklIrtFZKeIPOLqmOwhIt4islVEFro6luKISC0R+UZE9th+x1e5OqbiiMhjts/BDhGZJSKlKyvpZCIyQ0ROisiOfM+FiMgyEdln+1rblTFeVESsr9k+C9tEZK6I1HJhiJcpLN58rz0hIkZEQh1xLI9IBCLiDbwL3AS0ByaISHvXRlWkHOBPxph2QG/gITeONb9HgN2uDsIObwJLjDFtgSjcOGYRCQf+CHQ3xnQEvIHxro3qCp8Agws89xSwwhjTClhhe+wOPuHKWJcBHY0xnYDfgL9WdFDF+IQr40VEIoBBwBFHHcgjEgHQE9hvjIkzxmQBs4GRLo6pUMaYY8aYLbbvz2OdqMJdG1XxRKQRMBT40NWxFEdEagLXAB8BGGOyjDFnXBpUyXyAaiLiAwQCiS6O5zLGmJ+BlAJPjwQ+tX3/KXBzRcZUlMJiNcb8aIzJsT1cD5SufrMTFfG7Bfgf8BfAYSN9PCURhAPx+R4n4OYnVwARaQp0AaJdHEpJ3sD6YOa5OI6SNAeSgI9tzVgfikiQq4MqijHmKPA61pXfMeCsMeZH10Zll/rGmGNgXdgA9Vwcj70mAYtdHURxRGQEcNQYE+vI/XpKIpBCnnPrcbMiUh34FnjUGHPO1fEURUSGASeNMZtdHYsdfICuwDRjTBfgAu7TbHEFW9v6SKAZEAYEicgdro2qahKRZ7CaZR23IryDiUgg8Azwd0fv21MSQQIQke9xI9zsFjs/EfHFSgIzjTHfuTqeElwNjBCRQ1hNbteLyBeuDalICUCCMebiHdY3WInBXQ0EDhpjkowx2cB3QB8Xx2SPEyLSEMD29aSL4ymWiNwNDANuN+49saoF1kVBrO3vrRGwRUQalHfHnpIINgKtRKSZiPhhdbjNd3FMhRIRwWrD3m2M+a+r4ymJMeavxphGxpimWL/XlcYYt7xqNcYcB+JFpI3tqQHALheGVJIjQG8RCbR9Lgbgxp3b+cwH7rZ9fzfwvQtjKZaIDAaeBEYYY9JcHU9xjDHbjTH1jDFNbX9vCUBX2+e6XDwiEdg6gx4GlmL9Ic0xxux0bVRFuhq4E+vKOsb2b4irg6pCpgIzRWQb0Bn4h2vDKZrtzuUbYAuwHevv1a1KIojILGAd0EZEEkTkXuBVYJCI7MMa3fKqK2O8qIhY3wFqAMtsf2vvuTTIfIqI1znHcu87IaWUUs7mEXcESimliqaJQCmlPJwmAqWU8nCaCJRSysNpIlBKKQ+niUCpAkQkN9/Q3RhHVqsVkaaFVZNUypV8XB2AUm4o3RjT2dVBKFVR9I5AKTuJyCER+ZeIbLD9a2l7vomIrLDVtF8hIo1tz9e31biPtf27WB7CW0Q+sK0z8KOIVHPZD6UUmgiUKky1Ak1D4/K9ds4Y0xNrRuobtufeAT6z1bSfCbxle/4tYLUxJgqrptHF2eytgHeNMR2AM8AYp/40SpVAZxYrVYCIpBpjqhfy/CHgemNMnK0w4HFjTB0ROQU0NMZk254/ZowJFZEkoJExJjPfPpoCy2yLtiAiTwK+xpiXK+BHU6pQekegVOmYIr4vapvCZOb7Phftq1MupolAqdIZl+/rOtv3a/l9CcnbgV9s368AHoBLazrXrKgglSoNvRJR6krVRCQm3+MlxpiLQ0j9RSQa6yJqgu25PwIzROTPWCug3WN7/hFguq1qZC5WUjjm7OCVKi3tI1DKTrY+gu7GmFOujkUpR9KmIaWU8nB6R6CUUh5O7wiUUsrDaSJQSikPp4lAKaU8nCYCpZTycJoIlFLKw/0/tppd1gkVkMUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"Loss\",\"Val Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"vgg16\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, 224, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6088/2171937213.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msaved_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\jujut\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"vgg16\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "saved_model = load_model('model.h5')\n",
    "\n",
    "output = saved_model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission.csv')\n",
    "\n",
    "for idx, row in enumerate(output):\n",
    "    index = np.argmax(row)\n",
    "    submission.at[idx, \"class\"] = index\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100212,
     "end_time": "2021-03-08T07:59:04.516059",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.415847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Feature Representations\n",
    "## 1.0. Example: Identify feature extractor\n",
    "Our example feature extractor doesn't actually do anything... It just returns the input:\n",
    "$$\n",
    "\\forall x : f(x) = x.\n",
    "$$\n",
    "\n",
    "It does make for a good placeholder and baseclass ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.108781,
     "end_time": "2021-03-08T07:59:04.725071",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.61629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IdentityFeatureExtractor:\n",
    "    \"\"\"A simple function that returns the input\"\"\"\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.134288,
     "end_time": "2021-03-08T07:59:04.959911",
     "exception": false,
     "start_time": "2021-03-08T07:59:04.825623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1. Baseline 1: HOG feature extractor/Scale Invariant Feature Transform\n",
    "Possible improvements:\n",
    "- grid search to study the best parameterz on a validation set\n",
    "- preprocessing (normalization, ecc.)\n",
    "\n",
    "For HOG/SIFT explanation link on the assignment pdf.\n",
    "\n",
    "HOG performances with default parameters : 0.84581 (with GaussianBlur)\n",
    "SIFT performances with default parameters : 0.69913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.110122,
     "end_time": "2021-03-08T07:59:05.171171",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.061049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HOGFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, blockSize = (40,40), blockStride = (20,20), cellSize = (20,20), nbins = 18, derivAperture = 1\n",
    ", winSigma = -1, histogramNormType = 0, L2HysThreshold = 0.2, gammaCorrection = 1, nlevels = 64, signedGradients = True):\n",
    "        self.blockSize = blockSize\n",
    "        self.blockStride = blockStride\n",
    "        self.cellSize = cellSize\n",
    "        self.nbins = nbins\n",
    "        self.derivAperture = derivAperture\n",
    "        self.winSigma = winSigma\n",
    "        self.histogramNormType = histogramNormType \n",
    "        self.L2HysThreshold = L2HysThreshold\n",
    "        self.gammaCorrection = gammaCorrection\n",
    "        self.nlevels = nlevels\n",
    "        self.signedGradients = signedGradients         \n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.astype(np.uint8)\n",
    "        hog = cv2.HOGDescriptor((X.shape[1], X.shape[2]), self.blockSize, self.blockStride, self.cellSize, self.nbins,\n",
    "                                self.derivAperture, self.winSigma, self.histogramNormType,self.L2HysThreshold,\n",
    "                                self.gammaCorrection, self.nlevels, self.signedGradients)\n",
    "        descriptors = []\n",
    "        resultimage = np.zeros((FACE_SIZE[0], FACE_SIZE[1]))\n",
    "\n",
    "        for x in X:\n",
    "            img = cv2.GaussianBlur(x, (5,5), 0)\n",
    "            descriptors.append(hog.compute(img))\n",
    "\n",
    "        descriptors = np.array(descriptors)\n",
    "        descriptors = np.resize(descriptors, (descriptors.shape[0], descriptors.shape[1]))\n",
    "        return descriptors   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIFTFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, nfeatures=200, nOctaveLayers=3, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6, n_clusters = 200):\n",
    "        self.nfeatures = nfeatures\n",
    "        self.nOctaveLayers = nOctaveLayers\n",
    "        self.contrastThreshold = contrastThreshold\n",
    "        self.edgeThreshold = edgeThreshold \n",
    "        self.sigma = sigma\n",
    "        self.flag = 1 # 1 for train set, 0 for test \n",
    "        self.n_clusters = n_clusters\n",
    "        self.bow = []\n",
    "    \n",
    "    def extract_sift_features(self, list_image):\n",
    "\n",
    "        image_descriptors = []\n",
    "        sift = cv2.SIFT_create()\n",
    "        for image in list_image:        \n",
    "            _, descriptor = sift.detectAndCompute(image, None)\n",
    "            image_descriptors.append(descriptor)\n",
    "\n",
    "        return image_descriptors\n",
    "\n",
    "    def kmean_bow(self, all_descriptors, n_clusters):\n",
    "        bow_dict = []\n",
    "\n",
    "        kmeans = KMeans(n_clusters = n_clusters)\n",
    "        kmeans.fit(all_descriptors)\n",
    "\n",
    "        bow_dict = kmeans.cluster_centers_\n",
    "        self.bow = bow_dict\n",
    "        \n",
    "        return bow_dict\n",
    "\n",
    "    def create_feature_bow(self, image_descriptors, BoW, n_clusters):\n",
    "\n",
    "        X_features = []\n",
    "\n",
    "        for i in range(len(image_descriptors)):\n",
    "            features = np.zeros(n_clusters)\n",
    "\n",
    "            if image_descriptors[i] is not None:\n",
    "                distance = cdist(image_descriptors[i], BoW)\n",
    "\n",
    "                argmin = np.argmin(distance, axis = 1)\n",
    "\n",
    "                for j in argmin:\n",
    "                    features[j] += 1\n",
    "            X_features.append(features)\n",
    "\n",
    "        return X_features\n",
    "\n",
    "        \n",
    "    def transform(self, X):  \n",
    "        \n",
    "        X = X.astype(np.uint8)\n",
    "        \n",
    "        image_descriptors = self.extract_sift_features(X)\n",
    "        \n",
    "        # train mode\n",
    "        if self.flag == 1: \n",
    "            all_descriptors = []\n",
    "            for descriptor in image_descriptors:\n",
    "                if descriptor is not None:\n",
    "                    for des in descriptor:\n",
    "                        all_descriptors.append(des)\n",
    "\n",
    "            BoW = self.kmean_bow(all_descriptors, self.n_clusters)\n",
    "            \n",
    "            self.flag = 0 #switch to test mode\n",
    "            \n",
    "            return self.create_feature_bow(image_descriptors, BoW, self.n_clusters)\n",
    "        \n",
    "        # test mode\n",
    "        elif self.flag == 0:\n",
    "            return self.create_feature_bow(image_descriptors, self.bow, self.n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100377,
     "end_time": "2021-03-08T07:59:05.372401",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.272024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.1. t-SNE Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.100308,
     "end_time": "2021-03-08T07:59:05.57403",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.473722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "hog = HOGFeatureExtractor()\n",
    "\n",
    "X_embedded = TSNE().fit_transform(hog.transform(train_X))\n",
    "\n",
    "plt.ylim([-200, 200])\n",
    "plt.xlim([-200,200])\n",
    "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=train_y, legend='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100596,
     "end_time": "2021-03-08T07:59:05.775686",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.67509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1.2. Discussion\n",
    "Class 1 and 2 are well separated. Difficult discrimination between class 0-1 and 0-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101426,
     "end_time": "2021-03-08T07:59:05.978236",
     "exception": false,
     "start_time": "2021-03-08T07:59:05.87681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2. Baseline 2: PCA feature extractor\n",
    "\n",
    "Number of features can be determined by n_components (which depends on the number of images in the training data) or the percentage of variance to be retained.\n",
    "\n",
    "Not many parameters can be improved.\n",
    "\n",
    "PCA performances with default parameters : 0.77843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.111032,
     "end_time": "2021-03-08T07:59:06.191215",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.080183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PCAFeatureExtractor(IdentityFeatureExtractor):\n",
    "    \n",
    "    def __init__(self, train_X, n_components = None, var = 0.95):\n",
    "        train_X = train_X.astype(np.uint8)\n",
    "        gray_train_X = []\n",
    "        \n",
    "        #code to ensure grayscale conversion\n",
    "        if len(train_X) > 3 and train_X.shape[3] > 1:\n",
    "            for x in train_X:\n",
    "                gray_train_X.append(cv2.cvtColor(x, cv2.COLOR_BGR2GRAY))\n",
    "                \n",
    "        gray_train_X = np.array(gray_train_X)\n",
    "        \n",
    "        self.n_components = n_components\n",
    "        self.variance = var\n",
    "        #mean image computation\n",
    "        mean_image = gray_train_X.mean(axis=0)\n",
    "        #centered_data computation\n",
    "        centered_train = gray_train_X - mean_image\n",
    "        \n",
    "        m = train_X.shape[0]\n",
    "        d = FACE_SIZE[0] * FACE_SIZE[1]\n",
    "        images_matrix = np.reshape(centered_train, (m, d))\n",
    "        if n_components == None:\n",
    "            self.pca = PCA(n_components=var, svd_solver=\"full\", whiten=True).fit(images_matrix)\n",
    "        else:\n",
    "            self.pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(images_matrix)\n",
    "            \n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.astype(np.uint8)        \n",
    "        gray_X = []\n",
    "        \n",
    "        #code to ensure grayscale conversion\n",
    "        if len(X.shape) == 4:\n",
    "            m = X.shape[0]\n",
    "            if X.shape[3] > 1:\n",
    "                for x in X:\n",
    "                    gray_X.append(cv2.cvtColor(x, cv2.COLOR_BGR2GRAY))\n",
    "            else:\n",
    "                gray_X = np.array(X)\n",
    "            \n",
    "            gray_X = np.array(gray_X)\n",
    "        else:\n",
    "            if len(X.shape) > 2 and X.shape[2] > 1:\n",
    "                 gray_X = cv2.cvtColor(X, cv2.COLOR_BGR2GRAY)\n",
    "            m = 1\n",
    "        d = FACE_SIZE[0] * FACE_SIZE[1]\n",
    "        gray_X = np.reshape(gray_X, (m, d))\n",
    "        return self.pca.transform(gray_X)\n",
    "        \n",
    "    def inverse_transform(self, X):\n",
    "        data = self.pca.inverse_transform(X)\n",
    "        n_images = int(data.size / (FACE_SIZE[0]*FACE_SIZE[1]))\n",
    "        \n",
    "        data = np.array(data, dtype=np.uint8)\n",
    "        \n",
    "        if n_images > 1:\n",
    "            return np.reshape(data, (n_images, FACE_SIZE[0], FACE_SIZE[1]))\n",
    "        return np.reshape(data, (FACE_SIZE[0], FACE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.100881,
     "end_time": "2021-03-08T07:59:06.392861",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.29198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2.1. Eigenface Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_X[9]\n",
    "\n",
    "max_eigenvectors = train_X.shape[0]\n",
    "\n",
    "used_eigenvalues = np.linspace(10, max_eigenvectors, int(max_eigenvectors/10), dtype=int)[::-1]\n",
    "\n",
    "eigenfaces_images = np.zeros((int(max_eigenvectors/10), FACE_SIZE[0], FACE_SIZE[1], 3))\n",
    "\n",
    "for components in used_eigenvalues:\n",
    "    pca = PCAFeatureExtractor(train_X, components)\n",
    "    reduced_data = pca.transform(image)\n",
    "    reconstructed_data = pca.inverse_transform(reduced_data)\n",
    "    idx = int((max_eigenvectors - components) / 10)\n",
    "    eigenfaces_images[idx] = cv2.cvtColor(reconstructed_data, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "plot_image_sequence(eigenfaces_images, n=eigenfaces_images.shape[0], imgs_per_row=10)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101263,
     "end_time": "2021-03-08T07:59:06.797448",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.696185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2.2. Feature Space Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.101801,
     "end_time": "2021-03-08T07:59:07.000598",
     "exception": false,
     "start_time": "2021-03-08T07:59:06.898797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCAFeatureExtractor(train_X, 2)\n",
    "\n",
    "reduced_data = pca.transform(train_X)\n",
    "\n",
    "plt.plot(reduced_data[:,0][train_y == 0], reduced_data[:,1][train_y == 0], 'o', label=\"False\")\n",
    "plt.plot(reduced_data[:,0][train_y == 1], reduced_data[:,1][train_y == 1], 'o', label=\"Jesse Eisemberg\")\n",
    "plt.plot(reduced_data[:,0][train_y == 2], reduced_data[:,1][train_y == 2], 'o', label=\"Mila Kunis\")\n",
    "plt.title(\"Projecting the images in the 2D eigenspace\")\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102099,
     "end_time": "2021-03-08T07:59:07.204783",
     "exception": false,
     "start_time": "2021-03-08T07:59:07.102684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2.3. Discussion\n",
    "\n",
    "Again class 1 and 2 are well separated, while discrimination is difficult between class 0-1 and 0-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.10088,
     "end_time": "2021-03-08T07:59:07.406787",
     "exception": false,
     "start_time": "2021-03-08T07:59:07.305907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Evaluation Metrics\n",
    "## 2.0. Example: Accuracy\n",
    "As example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.180116,
     "end_time": "2021-03-08T07:59:08.688561",
     "exception": false,
     "start_time": "2021-03-08T07:59:07.508445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.103749,
     "end_time": "2021-03-08T07:59:08.894358",
     "exception": false,
     "start_time": "2021-03-08T07:59:08.790609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Classifiers\n",
    "## 3.0. Example: The *'not so smart'* classifier\n",
    "This random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.113194,
     "end_time": "2021-03-08T07:59:09.110222",
     "exception": false,
     "start_time": "2021-03-08T07:59:08.997028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"Random classifier, draws a random sample based on class distribution observed \n",
    "    during training.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            Training set\n",
    "        y : array\n",
    "            Training set labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : RandomClassificationModel\n",
    "        \"\"\" \n",
    "        \n",
    "        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n",
    "        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Samples labels for the input data. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : tensor\n",
    "            dataset\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_star : array\n",
    "            'Predicted' labels\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(0)\n",
    "        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Baseline 1: SVM classifier\n",
    "Two binary classifiers (usually work better for SVM).\n",
    "\n",
    "Randomized search for the estimation of the best hyperparameters.\n",
    "\n",
    "RBF kernel to obtain non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassificationModel:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #Jesse classifier\n",
    "        Y_jesse = np.array(y)\n",
    "        Y_jesse[Y_jesse == 2] = 0\n",
    "        \n",
    "        param_grid = {\n",
    "            \"C\": loguniform(1e3, 1e5),\n",
    "            \"gamma\": loguniform(1e-4, 1e-1),\n",
    "        }\n",
    "        clf_jesse = RandomizedSearchCV(\n",
    "            SVC(kernel=\"rbf\", class_weight=\"balanced\", random_state=9), param_grid, n_iter=1000, cv=20, random_state=9\n",
    "        )\n",
    "        clf_jesse = clf_jesse.fit(X, Y_jesse)\n",
    "        self.C_jesse = clf_jesse.best_estimator_.C\n",
    "        self.gamma_jesse = clf_jesse.best_estimator_.gamma\n",
    "        self.clf_jesse = clf_jesse\n",
    "        \n",
    "        #Mila classifier\n",
    "        Y_mila = np.array(y)\n",
    "        Y_mila[Y_mila == 1] = 0\n",
    "        \n",
    "        clf_mila = RandomizedSearchCV(\n",
    "            SVC(kernel=\"rbf\", class_weight=\"balanced\", random_state=9), param_grid, n_iter=1000, cv=10, random_state=9\n",
    "        )\n",
    "        clf_mila = clf_mila.fit(X, Y_mila)\n",
    "        self.C_mila = clf_mila.best_estimator_.C\n",
    "        self.gamma_mila = clf_mila.best_estimator_.gamma\n",
    "        self.clf_mila = clf_mila\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_jesse = self.clf_jesse.predict(X)\n",
    "        y_mila = self.clf_mila.predict(X)\n",
    "        \n",
    "        y = np.zeros(y_jesse.size, dtype=int)\n",
    "        \n",
    "        y[np.logical_and(y_jesse == 1, y_mila == 0)] = 1\n",
    "        y[np.logical_and(y_jesse == 0, y_mila == 2)] = 2\n",
    "        \n",
    "        #random choice when both Mila and Jesse are predicted\n",
    "        y[np.logical_and(y_jesse == 1, y_mila == 2)] = np.random.randint(0,2)\n",
    "        for i in range(y.size):\n",
    "            if y_jesse[i] == 1 and y_mila[i] == 2:\n",
    "                print(\"ERROR: both Jesse and Mila labels are predicted\")\n",
    "                \n",
    "        y_with_four_labels = np.array(y, dtype=int)\n",
    "        y_with_four_labels[np.logical_and(y_jesse == 1, y_mila == 2)] = 3\n",
    "        \n",
    "        return y, y_with_four_labels\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101099,
     "end_time": "2021-03-08T07:59:09.31402",
     "exception": false,
     "start_time": "2021-03-08T07:59:09.212921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2. Baseline 2: Voting among SVM classifiers\n",
    "\n",
    "Voting among the three classifiers that use HOG, SIFT and PCA features.\n",
    "\n",
    "Surprisingly, it does not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.108542,
     "end_time": "2021-03-08T07:59:09.525054",
     "exception": false,
     "start_time": "2021-03-08T07:59:09.416512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingClassificationModel:\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        hog_features = X[0]\n",
    "        sift_features = X[1]\n",
    "        pca_features = X[2]\n",
    "        \n",
    "        self.hog_classifier = SVMClassificationModel()\n",
    "        self.hog_classifier.fit(hog_features, y)\n",
    "        \n",
    "        self.sift_classifier = SVMClassificationModel()\n",
    "        self.sift_classifier.fit(sift_features, y)\n",
    "        \n",
    "        self.pca_classifier = SVMClassificationModel()\n",
    "        self.pca_classifier.fit(pca_features, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        hog_features = X[0]\n",
    "        sift_features = X[1]\n",
    "        pca_features = X[2]\n",
    "        y_hog, y_four_hog = self.hog_classifier.predict(hog_features)\n",
    "        y_sift, y_four_sift = y_mila_sift = self.sift_classifier.predict(sift_features)\n",
    "        y_pca, y_four_pca = self.pca_classifier.predict(pca_features)\n",
    "        \n",
    "        self.c0 = self.c1 = self.c2 = self.c3 = self.c4 = 0\n",
    "        y_final = -1*np.zeros((y_hog.size), dtype=int)\n",
    "        \n",
    "        for i in range(y_hog.size):\n",
    "            values = np.array([y_four_hog[i], y_four_sift[i], y_four_pca[i]])\n",
    "            \n",
    "            n_zeros = np.where(values == 0, 1, 0).sum()\n",
    "            n_ones = np.where(values == 1, 1, 0).sum()\n",
    "            n_twos = np.where(values == 2, 1, 0).sum()\n",
    "            n_threes = np.where(values == 3, 1, 0).sum()\n",
    "            \n",
    "            if n_zeros >= 2:\n",
    "                y_final[i] = 0\n",
    "            elif n_ones >= 2:\n",
    "                y_final[i] = 1\n",
    "            elif n_twos >= 2:\n",
    "                y_final[i] = 2\n",
    "            elif n_threes >= 2:\n",
    "                self.c0 += 1\n",
    "                y_final[i] = y_hog[i]\n",
    "            elif n_ones == 1 and n_twos == 1 and n_zeros == 1:\n",
    "                self.c1 += 1\n",
    "                y_final[i] = y_hog[i]\n",
    "            elif n_ones == 1 and n_zeros == 1 and n_threes == 1:\n",
    "                self.c2 += 1\n",
    "                y_final[i] = y_hog[i]\n",
    "            elif n_zeros == 1 and n_twos == 1 and n_threes == 1:\n",
    "                self.c3 += 1\n",
    "                y_final[i] = y_hog[i]\n",
    "            elif n_ones == 1 and n_twos == 1 and n_threes == 1:\n",
    "                self.c4 += 1\n",
    "                y_final[i] = y_hog[i]\n",
    "                \n",
    "        return y_final\n",
    "    def __call__(self, X):\n",
    "        y = self.predict(X)\n",
    "        print(self.c0)\n",
    "        print(self.c1)\n",
    "        print(self.c2)\n",
    "        print(self.c3)\n",
    "        print(self.c4)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Baseline: My favorite classifer\n",
    "Up to now, the best performing classifier is the SVM with HOG features.\n",
    "\n",
    "A possible improvement can be the VGG ImageNet with Data Augmentation. As you can read on https://arxiv.org/pdf/1804.06655.pdf, this NN approach should be the best performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102942,
     "end_time": "2021-03-08T07:59:09.730342",
     "exception": false,
     "start_time": "2021-03-08T07:59:09.6274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Experiments\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "## 4.0. Example: basic pipeline\n",
    "The basic pipeline takes any input and samples a label based on the class label distribution of the training set. As expected the performance is very poor, predicting approximately 1/4 correctly on the training set. There is a lot of room for improvement but this is left to you ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.430319,
     "end_time": "2021-03-08T07:59:10.263691",
     "exception": false,
     "start_time": "2021-03-08T07:59:09.833372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = IdentityFeatureExtractor() \n",
    "classifier = RandomClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model = lambda X: classifier(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.114717,
     "end_time": "2021-03-08T07:59:10.480473",
     "exception": false,
     "start_time": "2021-03-08T07:59:10.365756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate performance of the model on the training set\n",
    "train_y_star = model(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.111828,
     "end_time": "2021-03-08T07:59:10.696438",
     "exception": false,
     "start_time": "2021-03-08T07:59:10.58461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_star = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. SVM model: basic pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. SVM model: HOGFeatureDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = HOGFeatureExtractor() \n",
    "classifier_hog = SVMClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier_hog.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model_hog = lambda X: classifier_hog(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of the model on the training set\n",
    "train_y_hog, _ = model_hog(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_hog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_hog, _ = model_hog(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. SVM model: SIFTFeatureDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = SIFTFeatureExtractor() \n",
    "classifier_sift = SVMClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier_sift.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model_sift = lambda X: classifier_sift(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate performance of the model on the training set\n",
    "train_y_sift, _ = model_sift(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_sift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_sift, _ = model_sift(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. SVM model: PCAFeatureDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = PCAFeatureExtractor(train_X) \n",
    "classifier_pca = SVMClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier_pca.fit(feature_extractor(train_X), train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model_pca = lambda X: classifier_pca(feature_extractor(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate performance of the model on the training set\n",
    "train_y_pca, _ = model_pca(train_X)\n",
    "\n",
    "\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n",
    "    accuracy_score(train_y, train_y_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y_pca, _ = model_pca(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Voting model: voting among SVM classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_hog = HOGFeatureExtractor() \n",
    "feature_extractor_sift = SIFTFeatureExtractor() \n",
    "feature_extractor_pca = PCAFeatureExtractor(train_X) \n",
    "\n",
    "classifier = VotingClassificationModel()\n",
    "\n",
    "# train the model on the features\n",
    "classifier.fit([feature_extractor_hog(train_X), feature_extractor_sift(train_X), feature_extractor_pca(train_X)], train_y)\n",
    "\n",
    "# model/final pipeline\n",
    "model = lambda X: classifier([feature_extractor_hog(X), feature_extractor_sift(X), feature_extractor_pca(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels for the test set \n",
    "test_y = model(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.103853,
     "end_time": "2021-03-08T07:59:10.903341",
     "exception": false,
     "start_time": "2021-03-08T07:59:10.799488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Publishing best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.120392,
     "end_time": "2021-03-08T07:59:11.127762",
     "exception": false,
     "start_time": "2021-03-08T07:59:11.00737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = test.copy().drop('img', axis = 1)\n",
    "submission['class'] = test_y_hog\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.122516,
     "end_time": "2021-03-08T07:59:11.356409",
     "exception": false,
     "start_time": "2021-03-08T07:59:11.233893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.116655,
     "end_time": "2021-03-08T07:59:11.577703",
     "exception": false,
     "start_time": "2021-03-08T07:59:11.461048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Discussion\n",
    "...\n",
    "\n",
    "In summary we contributed the following: \n",
    "* \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
